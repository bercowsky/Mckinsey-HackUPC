{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2180d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d9a9d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "def load_data():\n",
    "    geo_params = pd.read_csv('../Data/geo_params.csv')\n",
    "    sales = pd.read_csv('../Data/sales.csv')\n",
    "    sku = pd.read_csv('../Data/sku.csv')\n",
    "    test = pd.read_csv('../Data/test.csv')\n",
    "    \n",
    "    ## Transform dataset\n",
    "    \n",
    "    sales['date'] = pd.to_datetime(sales['date'])\n",
    "    \n",
    "    # Fill null values with 0\n",
    "    sales['price'].fillna(0, inplace=True)\n",
    "    sales['sales'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Normalize data\n",
    "    sales[['price','sales']] = minmax_scale(sales[['price','sales']])\n",
    "    \n",
    "    sku.dropna(subset = ['Category'], inplace=True)\n",
    "    sku['Category'] = [c.split(',') for c in sku['Category']]\n",
    "    #print(sku['Category'])\n",
    "    \n",
    "    del sku['Units'] # Null field\n",
    "    \n",
    "    return geo_params, sales, sku, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9cb00e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['import' 'light' 'own production' 'parbaking' 'plain'\n",
      " 'slightly sparkling' 'sparkling' 'still' 'toast-type' 'white mold'\n",
      " 'Avocado' 'Banana' 'Ciabatta' 'Fancy strudel with poppy stuff'\n",
      " 'Flavored baguette loaf baked in the hearth'\n",
      " 'Flavored wheat bread baked in the hearth'\n",
      " 'Grain bread roll baked in the hearth' 'Grapefruit' 'Kiwi' 'Lemon' 'Lime'\n",
      " 'Mandarin' 'Mango' 'Milk cakes' 'Milk desserts' 'Orange'\n",
      " 'Plain baguette loaf' 'Plain baguette loaf baked in the hearth'\n",
      " 'Plain bread roll baked in the hearth' 'Plain croissant'\n",
      " 'Plain fancy cake' 'Plain long loaf' 'Plain rye-wheat bread'\n",
      " 'Plain wheat bread' 'Pomegranate'\n",
      " 'Rye-wheat grain bread baked in the hearth' 'Semi-hard bulk cheese'\n",
      " 'Semi-hard coarse-pored cheese' 'Semi-hard layered cheese'\n",
      " 'Small fancy bread with berry stuff' 'Small flavored bread roll'\n",
      " 'Soft cheese' 'Sweet bun' 'Water' 'Wheat bran bread baked in the hearth'\n",
      " 'Yoghurts']\n"
     ]
    }
   ],
   "source": [
    "[geo_params,sales,sku,test] = load_data()\n",
    "# Categories to train the model\n",
    "cat_aux = (np.concatenate(sku['Category'].to_numpy()).ravel())\n",
    "Categories = np.char.strip(np.unique(cat_aux))\n",
    "print(Categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba9d83db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   geoCluster  cityId\n",
      "0          21       1\n",
      "1          47       1\n",
      "2          48       1\n",
      "3          92       1\n",
      "4         112       1\n"
     ]
    }
   ],
   "source": [
    "# Geo_params Headers\n",
    "print(geo_params.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5a67d265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ID  geoCluster    SKU       date     price     sales\n",
      "0  RR27956459          92  32485 2021-07-05  0.005284  0.004994\n",
      "1  RR27956474          92  32549 2021-07-05  0.012335  0.000624\n",
      "2  RR27956489         112  32485 2021-05-27  0.006263  0.004619\n",
      "3  RR27956490         112  32485 2021-05-28  0.000000  0.000000\n",
      "4  RR27956491         112  32485 2021-05-29  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Sales Headers\n",
    "print(sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ba113e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SKU                      Category  \\\n",
      "0     24                 [Pomegranate]   \n",
      "1    208           [Water,  sparkling]   \n",
      "2   1008  [Water,  import,  sparkling]   \n",
      "3  16649               [Water,  still]   \n",
      "4  20872           [Water,  sparkling]   \n",
      "\n",
      "                                            Type  brandId  lagerUnitQuantity  \\\n",
      "0            Tropical fruit — Pomegranate— Plain      NaN                1.0   \n",
      "1  Therapeutic-table water — PET — from 1 to 2 L   1241.0                1.5   \n",
      "2       Therapeutic-table water — Import — Glass   1241.0                0.5   \n",
      "3      Table water — PET — from 1 to 2 L — Still   1241.0                1.5   \n",
      "4  Therapeutic-table water — PET — from 1 to 2 L   2693.0                1.5   \n",
      "\n",
      "   trademark  countryOfOrigin            Group  \n",
      "0        NaN              NaN  Tropical fruits  \n",
      "1     3670.0              1.0    Mineral water  \n",
      "2     4970.0             14.0    Mineral water  \n",
      "3     1323.0              1.0    Mineral water  \n",
      "4     4384.0              1.0    Mineral water  \n"
     ]
    }
   ],
   "source": [
    "# Sku Headers\n",
    "print(sku.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0fa5ddf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ID  geoCluster    SKU        date  price_filled  sales\n",
      "0  RR27956447          21  32485  2021-07-08         39.69    NaN\n",
      "1  RR27956448          21  32485  2021-07-09         39.69    NaN\n",
      "2  RR27956449          21  32485  2021-07-10         39.69    NaN\n",
      "3  RR27956450          21  32485  2021-07-11         39.69    NaN\n",
      "4  RR27956451          21  32485  2021-07-12         39.69    NaN\n"
     ]
    }
   ],
   "source": [
    "# Test Headers\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d72316da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge table sales and sku\n",
    "\n",
    "df = pd.merge(sales, sku, on='SKU', how='inner')\n",
    "df = df.explode('Category')\n",
    "df['Category'] = df['Category'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9590d8c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                           object\n",
      "geoCluster                    int64\n",
      "SKU                           int64\n",
      "date                 datetime64[ns]\n",
      "price                       float64\n",
      "sales                       float64\n",
      "Category                     object\n",
      "Type                         object\n",
      "brandId                     float64\n",
      "lagerUnitQuantity           float64\n",
      "trademark                   float64\n",
      "countryOfOrigin             float64\n",
      "Group                        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Types of columns\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cd6422a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test set\n",
    "\n",
    "def load_sets(data):\n",
    "    X = data[['date', 'price']].copy()\n",
    "    y = data[['sales']].copy()\n",
    "    trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3583a886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import\n",
      "0                                     [Banana]\n",
      "1                                     [Banana]\n",
      "2                                     [Banana]\n",
      "3                                     [Banana]\n",
      "4                                     [Banana]\n",
      "                          ...                 \n",
      "4121838    [Plain fancy cake,  own production]\n",
      "4121839    [Plain fancy cake,  own production]\n",
      "4121840    [Plain fancy cake,  own production]\n",
      "4121841    [Plain fancy cake,  own production]\n",
      "4121842    [Plain fancy cake,  own production]\n",
      "Name: Category, Length: 4121843, dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [109]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m category]\n\u001b[0;32m----> 7\u001b[0m trainX, trainY, testX, testY \u001b[38;5;241m=\u001b[39m \u001b[43mload_sets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36mload_sets\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 6\u001b[0m trainX, testX, trainY, testY \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainX, trainY, testX, testY\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2420\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2417\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2419\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2420\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2422\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2098\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2095\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2098\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2099\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2102\u001b[0m     )\n\u001b[1;32m   2104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Train 1 model for each category\n",
    "for category in Categories:\n",
    "    print(category.strip())\n",
    "    print(df['Category'])\n",
    "    data = df.loc[df['Category'] == category]\n",
    "    \n",
    "    trainX, trainY, testX, testY = load_sets(data)\n",
    "    \n",
    "    \n",
    "#print(category, trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "#print(Categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f320b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81e5da25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainX \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m      2\u001b[0m trainY \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m      4\u001b[0m testX \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "trainX = train.iloc[:, :-1].to_numpy()\n",
    "trainY = train.iloc[:, -1].to_numpy()\n",
    "\n",
    "testX = test.iloc[:, :-1].to_numpy()\n",
    "testY = test.iloc[:, -1].to_numpy()\n",
    "\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25423e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "\n",
    "look_back = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb833f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
